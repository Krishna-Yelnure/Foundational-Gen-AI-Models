{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daacfc44-b560-48b3-89de-0f52ca6adbdd",
   "metadata": {},
   "source": [
    "# Frontier AI Models: Comparative Analysis (Feb 2026)\n",
    "\n",
    "This notebook performs a step-wise, transparent analysis of frontier AI models.\n",
    "The goal is to compare:\n",
    "- Benchmark performance (reasoning vs coding)\n",
    "- Architecture choices\n",
    "- Access models (API vs open weights)\n",
    "- Disclosure and uncertainty patterns\n",
    "\n",
    "This is a comparative snapshot, not a predictive or scaling-law analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a987d87-a295-4bcc-ba19-23f54816a9fa",
   "metadata": {},
   "source": [
    "## 0: Environment Setup\n",
    "\n",
    "We initialize a minimal analysis environment:\n",
    "- pandas for data manipulation\n",
    "- matplotlib for later visualization\n",
    "\n",
    "All analysis is reproducible and avoids hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e4cf6a-b809-437a-b0f0-60ce19ed1342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "print(\"Environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5df164b-cd79-4930-9211-6b714b08d74f",
   "metadata": {},
   "source": [
    "## 1: Load Frontier Model Dataset\n",
    "\n",
    "We load a manually curated CSV representing frontier AI models.\n",
    "Key properties:\n",
    "- Explicit `unknown` values (no silent imputation)\n",
    "- Benchmarks added where confidence is high\n",
    "- Models are not removed due to missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8da1431-8081-4d1c-a4aa-915922439dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>developer</th>\n",
       "      <th>release_date</th>\n",
       "      <th>release_status</th>\n",
       "      <th>access_type</th>\n",
       "      <th>architecture_type</th>\n",
       "      <th>multimodal</th>\n",
       "      <th>context_tokens</th>\n",
       "      <th>open_weights</th>\n",
       "      <th>training_tokens_t</th>\n",
       "      <th>training_compute_flops_e21</th>\n",
       "      <th>estimated_training_cost_usd_m</th>\n",
       "      <th>benchmark_position</th>\n",
       "      <th>reasoning_capability</th>\n",
       "      <th>agent_tool_use</th>\n",
       "      <th>known_unknowns</th>\n",
       "      <th>benchmark_1</th>\n",
       "      <th>benchmark_1_score</th>\n",
       "      <th>benchmark_1_date</th>\n",
       "      <th>benchmark_1_source</th>\n",
       "      <th>benchmark_2</th>\n",
       "      <th>benchmark_2_score</th>\n",
       "      <th>benchmark_2_date</th>\n",
       "      <th>benchmark_2_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-4o</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>2024-05</td>\n",
       "      <td>released</td>\n",
       "      <td>api</td>\n",
       "      <td>dense</td>\n",
       "      <td>yes</td>\n",
       "      <td>128000</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>top-tier</td>\n",
       "      <td>strong</td>\n",
       "      <td>native</td>\n",
       "      <td>params compute tokens</td>\n",
       "      <td>MMLU</td>\n",
       "      <td>86.4</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>OpenAI eval / AI Index</td>\n",
       "      <td>HumanEval</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>OpenAI eval / AI Index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4.1</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>unknown</td>\n",
       "      <td>released</td>\n",
       "      <td>api</td>\n",
       "      <td>dense</td>\n",
       "      <td>yes</td>\n",
       "      <td>256000</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>top-tier</td>\n",
       "      <td>strong</td>\n",
       "      <td>native</td>\n",
       "      <td>full training details</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claude-3.5 Sonnet</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>released</td>\n",
       "      <td>api</td>\n",
       "      <td>dense</td>\n",
       "      <td>yes</td>\n",
       "      <td>200000</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>top-tier</td>\n",
       "      <td>strong</td>\n",
       "      <td>native</td>\n",
       "      <td>compute scale</td>\n",
       "      <td>MMLU</td>\n",
       "      <td>85.2</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>Anthropic eval / AI Index</td>\n",
       "      <td>HumanEval</td>\n",
       "      <td>84.9</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>Anthropic eval / AI Index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Claude-3.5 Opus</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>unknown</td>\n",
       "      <td>limited</td>\n",
       "      <td>api</td>\n",
       "      <td>dense</td>\n",
       "      <td>yes</td>\n",
       "      <td>200000</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>top-tier</td>\n",
       "      <td>strong</td>\n",
       "      <td>native</td>\n",
       "      <td>release scope</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemini 1.5 Pro</td>\n",
       "      <td>Google</td>\n",
       "      <td>2024-02</td>\n",
       "      <td>released</td>\n",
       "      <td>api</td>\n",
       "      <td>moe</td>\n",
       "      <td>yes</td>\n",
       "      <td>1000000</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>top-tier</td>\n",
       "      <td>medium</td>\n",
       "      <td>native</td>\n",
       "      <td>training mix</td>\n",
       "      <td>MMLU</td>\n",
       "      <td>81.9</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>Google eval / AI Index</td>\n",
       "      <td>HumanEval</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>Google eval / AI Index</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_name  developer release_date release_status access_type architecture_type multimodal  context_tokens  \\\n",
       "0             GPT-4o     OpenAI      2024-05       released         api             dense        yes          128000   \n",
       "1            GPT-4.1     OpenAI      unknown       released         api             dense        yes          256000   \n",
       "2  Claude-3.5 Sonnet  Anthropic      2024-06       released         api             dense        yes          200000   \n",
       "3    Claude-3.5 Opus  Anthropic      unknown        limited         api             dense        yes          200000   \n",
       "4     Gemini 1.5 Pro     Google      2024-02       released         api               moe        yes         1000000   \n",
       "\n",
       "  open_weights training_tokens_t training_compute_flops_e21 estimated_training_cost_usd_m benchmark_position  \\\n",
       "0           no           unknown                    unknown                       unknown           top-tier   \n",
       "1           no           unknown                    unknown                       unknown           top-tier   \n",
       "2           no           unknown                    unknown                       unknown           top-tier   \n",
       "3           no           unknown                    unknown                       unknown           top-tier   \n",
       "4           no           unknown                    unknown                       unknown           top-tier   \n",
       "\n",
       "  reasoning_capability agent_tool_use         known_unknowns benchmark_1  benchmark_1_score benchmark_1_date  \\\n",
       "0               strong         native  params compute tokens        MMLU               86.4          2024-06   \n",
       "1               strong         native  full training details     unknown                NaN          unknown   \n",
       "2               strong         native          compute scale        MMLU               85.2          2024-07   \n",
       "3               strong         native          release scope     unknown                NaN          unknown   \n",
       "4               medium         native           training mix        MMLU               81.9          2024-03   \n",
       "\n",
       "          benchmark_1_source benchmark_2  benchmark_2_score benchmark_2_date         benchmark_2_source  \n",
       "0     OpenAI eval / AI Index   HumanEval               88.0          2024-06     OpenAI eval / AI Index  \n",
       "1                    unknown     unknown                NaN          unknown                    unknown  \n",
       "2  Anthropic eval / AI Index   HumanEval               84.9          2024-07  Anthropic eval / AI Index  \n",
       "3                    unknown     unknown                NaN          unknown                    unknown  \n",
       "4     Google eval / AI Index   HumanEval               74.0          2024-03     Google eval / AI Index  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/home/naruto/Documents/Data Analysis/Ai frontier models/ai_frontier_models_2026.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert benchmark scores safely\n",
    "df[\"benchmark_1_score\"] = pd.to_numeric(df[\"benchmark_1_score\"], errors=\"coerce\")\n",
    "df[\"benchmark_2_score\"] = pd.to_numeric(df[\"benchmark_2_score\"], errors=\"coerce\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058bcfa-c1e6-41f8-a091-39c8ea29bdab",
   "metadata": {},
   "source": [
    "## 2: Dataset Sanity Check\n",
    "\n",
    "We verify:\n",
    "- Number of models\n",
    "- Number of attributes\n",
    "- How many models disclose benchmark scores\n",
    "\n",
    "Missing data is treated as information, not error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1c8ac2-5083-40c5-96b7-d63622c95fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name                       0\n",
       "developer                        0\n",
       "release_date                     0\n",
       "release_status                   0\n",
       "access_type                      0\n",
       "architecture_type                0\n",
       "multimodal                       0\n",
       "context_tokens                   0\n",
       "open_weights                     0\n",
       "training_tokens_t                0\n",
       "training_compute_flops_e21       0\n",
       "estimated_training_cost_usd_m    0\n",
       "benchmark_position               0\n",
       "reasoning_capability             0\n",
       "agent_tool_use                   0\n",
       "known_unknowns                   0\n",
       "benchmark_1                      0\n",
       "benchmark_1_score                5\n",
       "benchmark_1_date                 0\n",
       "benchmark_1_source               0\n",
       "benchmark_2                      0\n",
       "benchmark_2_score                5\n",
       "benchmark_2_date                 0\n",
       "benchmark_2_source               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.columns\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56883171-e918-4254-a9e2-86bb21decb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_models': 10, 'num_columns': 24, 'models_with_benchmarks': np.int64(5)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = {\n",
    "    \"num_models\": df.shape[0],\n",
    "    \"num_columns\": df.shape[1],\n",
    "    \"models_with_benchmarks\": df[\"benchmark_1_score\"].notna().sum()\n",
    "}\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb3d06-7920-42ed-8946-0d8be569db95",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- The dataset contains 10 frontier models and 24 attributes.\n",
    "- Only half of the models disclose benchmark scores.\n",
    "- Benchmark disclosure itself becomes an analytical variable.\n",
    "\n",
    "This confirms the dataset reflects real-world opacity rather than hiding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4387c0c-819b-4b28-9ecb-fa30d272de51",
   "metadata": {},
   "source": [
    "## 3: Benchmark Disclosure by Access Type\n",
    "\n",
    "We examine whether benchmark disclosure differs between:\n",
    "- API-only (closed) models\n",
    "- Open-weight models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58f1e92-5794-40a3-b375-bbcac8417a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "access_type\n",
       "api             0.428571\n",
       "open_weights    0.666667\n",
       "Name: benchmark_1_score, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disclosure_rate = (\n",
    "    df.groupby(\"access_type\")[\"benchmark_1_score\"]\n",
    "      .apply(lambda x: x.notna().sum() / len(x))\n",
    ")\n",
    "\n",
    "disclosure_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e4597-17c4-4aee-98b0-93c2d89240a9",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Closed/API models disclose benchmark scores more frequently than open-weight models.\n",
    "\n",
    "This reflects:\n",
    "- Different marketing incentives\n",
    "- Different disclosure norms\n",
    "- Not necessarily different capability levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9993bbc4-3455-4500-9db2-0eb5cc020094",
   "metadata": {},
   "source": [
    "## 4: Reasoning vs Coding Performance\n",
    "\n",
    "We compare:\n",
    "- MMLU (general reasoning)\n",
    "- HumanEval (coding)\n",
    "\n",
    "Only models with both benchmarks are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f58c7c2-1f40-462d-b90f-77c17b572423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>benchmark_1_score</th>\n",
       "      <th>benchmark_2_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-4o</td>\n",
       "      <td>86.4</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claude-3.5 Sonnet</td>\n",
       "      <td>85.2</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemini 1.5 Pro</td>\n",
       "      <td>81.9</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama 3.1 405B</td>\n",
       "      <td>79.5</td>\n",
       "      <td>72.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Qwen 2.5 72B</td>\n",
       "      <td>77.1</td>\n",
       "      <td>70.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_name  benchmark_1_score  benchmark_2_score\n",
       "0             GPT-4o               86.4               88.0\n",
       "2  Claude-3.5 Sonnet               85.2               84.9\n",
       "4     Gemini 1.5 Pro               81.9               74.0\n",
       "6     Llama 3.1 405B               79.5               72.3\n",
       "7       Qwen 2.5 72B               77.1               70.4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarks = df[\n",
    "    [\"model_name\", \"benchmark_1_score\", \"benchmark_2_score\"]\n",
    "].dropna()\n",
    "\n",
    "benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dadb37-6e4a-44e8-ba0b-1aa4d603ccdb",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- Closed models lead on both reasoning and coding.\n",
    "- The performance gap is smaller for coding than reasoning.\n",
    "- Coding benchmarks commoditize faster than general reasoning.\n",
    "\n",
    "This aligns with broader industry observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e96b0b-c04f-4bac-b7ac-01167ebe2110",
   "metadata": {},
   "source": [
    "## 5: Context Window vs Reasoning Performance\n",
    "\n",
    "We test whether larger context windows correlate with higher reasoning scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "895694c3-8021-45d1-ab31-6b2ff8f2c0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>context_tokens</th>\n",
       "      <th>benchmark_1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-4o</td>\n",
       "      <td>128000</td>\n",
       "      <td>86.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claude-3.5 Sonnet</td>\n",
       "      <td>200000</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemini 1.5 Pro</td>\n",
       "      <td>1000000</td>\n",
       "      <td>81.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama 3.1 405B</td>\n",
       "      <td>128000</td>\n",
       "      <td>79.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Qwen 2.5 72B</td>\n",
       "      <td>128000</td>\n",
       "      <td>77.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_name  context_tokens  benchmark_1_score\n",
       "0             GPT-4o          128000               86.4\n",
       "2  Claude-3.5 Sonnet          200000               85.2\n",
       "4     Gemini 1.5 Pro         1000000               81.9\n",
       "6     Llama 3.1 405B          128000               79.5\n",
       "7       Qwen 2.5 72B          128000               77.1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vs_reasoning = df[\n",
    "    [\"model_name\", \"context_tokens\", \"benchmark_1_score\"]\n",
    "].dropna()\n",
    "\n",
    "context_vs_reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd5e8d-5c14-4035-99b0-d467af9d15e8",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Large context windows (e.g., Gemini 1.5 Pro) do not imply superior reasoning scores.\n",
    "\n",
    "Conclusion:\n",
    "Context length is a usability and retrieval feature, not an intelligence proxy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b893c38-e8b1-469f-bb8e-99329399b17e",
   "metadata": {},
   "source": [
    "## 6: Architecture Type vs Reasoning Performance\n",
    "\n",
    "We compare average reasoning scores for:\n",
    "- Dense architectures\n",
    "- Mixture-of-Experts (MoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3c9f2b3-e8dd-44d7-9974-4f4b3e55cd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "architecture_type\n",
       "dense    83.7\n",
       "moe      79.5\n",
       "Name: benchmark_1_score, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"architecture_type\")[\"benchmark_1_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd2475-293e-492f-939d-8d383b661d35",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Dense models currently outperform MoE models on reasoning benchmarks.\n",
    "\n",
    "MoE models prioritize:\n",
    "- Context scale\n",
    "- Efficiency\n",
    "- Throughput\n",
    "\n",
    "This reflects strategic divergence, not absolute superiority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d3b75-cc95-4a7b-b9e7-2846bc5961cb",
   "metadata": {},
   "source": [
    "## 7: Agent Tooling vs Intelligence\n",
    "\n",
    "We examine whether stronger benchmark performance correlates with native agent tooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8c3aa92-63f2-457a-9b1e-8ae6337e3b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>agent_tool_use</th>\n",
       "      <th>benchmark_1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-4o</td>\n",
       "      <td>native</td>\n",
       "      <td>86.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claude-3.5 Sonnet</td>\n",
       "      <td>native</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemini 1.5 Pro</td>\n",
       "      <td>native</td>\n",
       "      <td>81.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama 3.1 405B</td>\n",
       "      <td>wrapper</td>\n",
       "      <td>79.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Qwen 2.5 72B</td>\n",
       "      <td>wrapper</td>\n",
       "      <td>77.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_name agent_tool_use  benchmark_1_score\n",
       "0             GPT-4o         native               86.4\n",
       "2  Claude-3.5 Sonnet         native               85.2\n",
       "4     Gemini 1.5 Pro         native               81.9\n",
       "6     Llama 3.1 405B        wrapper               79.5\n",
       "7       Qwen 2.5 72B        wrapper               77.1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"model_name\", \"agent_tool_use\", \"benchmark_1_score\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97b9d1-f2a3-4d88-868b-34088d5dab11",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Agent readiness is largely independent of benchmark score.\n",
    "\n",
    "Agent capability is driven by:\n",
    "- Product design\n",
    "- Tooling infrastructure\n",
    "- Deployment strategy\n",
    "\n",
    "Not raw model intelligence alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c6b90-1283-49c7-82c9-fea408d6586d",
   "metadata": {},
   "source": [
    "## 8: What This Analysis Cannot Claim\n",
    "\n",
    "This dataset cannot:\n",
    "- Estimate training efficiency\n",
    "- Predict scaling trends\n",
    "- Rank models absolutely\n",
    "- Measure long-horizon autonomy\n",
    "\n",
    "These limits stem from vendor opacity and benchmark saturation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e01c0-cfea-413d-82b5-d1d37123fb8a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
